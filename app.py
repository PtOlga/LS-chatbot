import os
import streamlit as st
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

os.environ["USER_AGENT"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"


# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    temperature=0.6,
    api_key=GROQ_API_KEY
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embeddings_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large-instruct")

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
def load_english_pages(urls):
    english_docs = []
    
    for url in urls:
        # –ï—Å–ª–∏ –≤ URL –Ω–µ—Ç —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "/ru", "/ar"), —Ç–æ —ç—Ç–æ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if not any(lang in url for lang in ["/ru", "/ar", "/ch"]):  
            loader = WebBaseLoader([url])
            documents = loader.load()
            english_docs.extend(documents)  # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ —Å–ø–∏—Å–æ–∫
    
    return english_docs

# –ü—Ä–∏–º–µ—Ä URL, –≥–¥–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
urls = ["https://status.law/about", "https://status.law/ru/about", "https://status.law/ar/contact"]
documents = load_english_pages(urls)

# –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
vector_store = InMemoryVectorStore.from_documents(chunks, embeddings_model)
retriever = vector_store.as_retriever()

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –±–æ—Ç–∞
template = """
You are a helpful legal assistant that answers questions based on information from status.law.

Answer accurately and concisely.

Question: {question}

Only use the provided context to answer the question.

Context: {context}
"""

prompt = PromptTemplate.from_template(template)

# –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
def format_history(message_history):
    formatted = ""
    for msg in message_history:
        formatted += f"User: {msg['question']}\nBot: {msg['answer']}\n\n"
    return formatted

message_history = []

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
st.set_page_config(page_title="Legal Chatbot", page_icon="ü§ñ")
st.title("ü§ñ Legal Chatbot")
st.write("–≠—Ç–æ—Ç –±–æ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å —Å–∞–π—Ç–∞ status.law.")

# –ü–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ –≤–æ–ø—Ä–æ—Å–∞
user_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    if user_input:
        chain = {
            "context": retriever,
            "question": RunnablePassthrough(),
            "chat_history": RunnableLambda(lambda x: format_history(message_history))
        } | prompt | llm | StrOutputParser()

        result = chain.invoke(user_input)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        message_history.append({"question": user_input, "answer": result})

        # –í—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç
        st.write("**–ë–æ—Ç:**", result)
